import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

class LLMManager:
    """
    LM Studio(ë¡œì»¬) ë˜ëŠ” OpenAI í˜¸í™˜ ì„œë²„ì— ì—°ê²°í•˜ëŠ” í´ë˜ìŠ¤ì…ë‹ˆë‹¤.
    """
    def __init__(self):
        self.base_url = os.getenv("LM_STUDIO_URL", "http://localhost:1234/v1")
        self.api_key = "lm-studio"  # ë¡œì»¬ ì„œë²„ëŠ” í‚¤ê°€ í•„ìš” ì—†ì§€ë§Œ í˜•ì‹ìƒ ì…ë ¥

    def get_model(self, model_name="local-model"):
        """
        ChatOpenAI ê°ì²´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤. 
        LM Studioì—ì„œ ë¡œë“œí•œ ëª¨ë¸ì˜ ì´ë¦„ì´ ë‹¤ë¥¼ ê²½ìš° model_nameì„ ìˆ˜ì •í•˜ì„¸ìš”.
        """
        return ChatOpenAI(
            base_url=self.base_url,
            api_key=self.api_key,
            model=model_name,
            temperature=0  # KPI ë‹µë³€ì€ ì •í™•í•´ì•¼ í•˜ë¯€ë¡œ ì°½ì˜ì„±ì„ ë‚®ì¶¤(0)
        )

if __name__ == "__main__":
    # ì—°ê²° í…ŒìŠ¤íŠ¸
    manager = LLMManager()
    llm = manager.get_model()
    try:
        response = llm.invoke("ì•ˆë…•? ë„ˆëŠ” ì–´ë–¤ ì¼ì„ í•  ìˆ˜ ìˆì–´?")
        print("ğŸ¤– LLM ì‘ë‹µ:", response.content)
    except Exception as e:
        print("âŒ ì—°ê²° ì‹¤íŒ¨! LM Studioì˜ 'Start Server' ë²„íŠ¼ì„ ëˆŒë €ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
        print("ì—ëŸ¬ ë‚´ìš©:", e)